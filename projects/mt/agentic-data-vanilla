import nest_asyncio

nest_asyncio.apply()

import asyncio
import ollama
import json
from typing import Dict, List, Any, Optional
import pandas as pd
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod
from datetime import datetime
import logging
import seaborn as sns
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMInterface:
    """Interface for interacting with Ollama LLM"""

    def __init__(self, model_name: str = "llama2:latest"):
        self.model_name = model_name

    async def generate_analysis(self, prompt: str, context: Optional[Dict] = None) -> str:
        try:
            response = ollama.generate(
                model=self.model_name, prompt=prompt, context=context
            )
            return response["response"]
        except Exception as e:
            logging.error(f"Error generating LLM response: {str(e)}")
            return "Error generating analysis"

class ReportAgent(ABC):
    """Base class for report-generating agents"""

    def __init__(self, name: str):
        self.name = name
        self.created_at = datetime.now()
        self.llm = LLMInterface()

    @abstractmethod
    async def process_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Process the data and generate report components"""
        pass

    def format_output(self, results: Dict[str, Any]) -> str:
        """Format the results into a readable report"""
        return (
            f"Report generated by {self.name}\nTimestamp: {datetime.now()}\n\n{results}"
        )

class SchemaAnalysisAgent(ReportAgent):
    """Agent for analyzing the schema and data types of the input data."""

    async def process_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        schema_info = {}

        # Add column names and data types
        schema_info['columns'] = {
            col: str(data[col].dtype) for col in data.columns
        }

        # Basic descriptive statistics
        schema_info['stats'] = data.describe(include='all', datetime_is_numeric=True).to_dict()

        # Generate LLM analysis for schema description
        schema_prompt = f"""
        You are a data analyst tasked with explaining the schema of a dataset. 
        The dataset has the following columns and data types:

        {json.dumps(schema_info['columns'], indent=2)}

        Provide a concise description of the dataset, including:
        1. The overall purpose of the dataset.
        2. A brief explanation of each column and its meaning.
        3. Any potential relationships between columns.

        Additionally, here are some basic statistics about the columns:
        {json.dumps(schema_info['stats'], indent=2)}
        
        Use these statistics to further explain the data in each column.
        Be clear and concise, using natural language that a non-technical user can understand.
        """

        llm_schema_description = await self.llm.generate_analysis(schema_prompt)

        return {
            'schema': schema_info,
            'schema_description': llm_schema_description
        }

class DataTypeAnalysisAgent(ReportAgent):
    """Agent for analyzing data types and characteristics of the data."""

    async def process_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        type_info = {}

        for col in data.columns:
            type_info[col] = {}

            # Data type
            type_info[col]['dtype'] = str(data[col].dtype)

            # Uniqueness
            type_info[col]['unique_values'] = data[col].nunique()
            type_info[col]['is_unique'] = data[col].is_unique

            # Missing values
            type_info[col]['missing_percentage'] = (data[col].isnull().sum() / len(data)) * 100

            # Value ranges (numerical)
            if pd.api.types.is_numeric_dtype(data[col]):
                type_info[col]['min'] = data[col].min()
                type_info[col]['max'] = data[col].max()
                type_info[col]['mean'] = data[col].mean()
                type_info[col]['median'] = data[col].median()
            # Value counts (categorical)
            elif pd.api.types.is_string_dtype(data[col]):
                type_info[col]['value_counts'] = data[col].value_counts().head(10).to_dict()  # Top 10

        # Generate LLM analysis for data type description
        type_prompt = f"""
        You are a data analyst tasked with explaining the data types and characteristics of a dataset.

        Here is information about each column:
        {json.dumps(type_info, indent=2)}

        Provide a concise analysis for each column, including:
        1. An interpretation of its data type.
        2. Whether the column could be a unique identifier.
        3. The distribution of values (ranges for numerical, common values for categorical).
        4. The percentage of missing values and potential implications.

        Use natural language and be clear and concise.
        """

        llm_type_description = await self.llm.generate_analysis(type_prompt)

        return {
            'data_types': type_info,
            'type_description': llm_type_description,
        }

class NaturalLanguageQueryingAgent(ReportAgent):
    """Agent for handling natural language queries about the data."""

    def __init__(self, name: str):
        super().__init__(name)
        self.schema_agent = SchemaAnalysisAgent("Schema Analyzer")
        self.type_agent = DataTypeAnalysisAgent("Data Type Analyzer")

    async def process_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        # Get schema and data type information from other agents
        schema_results = await self.schema_agent.process_data(data.copy())
        type_results = await self.type_agent.process_data(data.copy())

        schema_info = schema_results['schema']
        schema_description = schema_results['schema_description']
        type_info = type_results['data_types']
        type_description = type_results['type_description']

        # Get user query (for now, let's assume it's hardcoded)
        # You would replace this with a mechanism to get input from the user
        user_query = input("Enter your natural language query: ")

        # Construct the prompt for the LLM
        query_prompt = f"""
        You are a data analyst who can translate natural language queries into Python code that operates on a pandas DataFrame.

        Here is the schema of the DataFrame:
        {json.dumps(schema_info, indent=2)}
        
        Schema Description:
        {schema_description}

        Here is information about the data types and characteristics of each column:
        {json.dumps(type_info, indent=2)}

        Data Type Description:
        {type_description}

        The user wants to know: "{user_query}"

        Generate Python code using the pandas library to answer this query.
        Assume the DataFrame is named 'data'.

        Example:
        If the user asks 'What is the average of the 'avg_wait_tm_nbr' column?', you should generate:
        ```python
        average_wait_time = data['avg_wait_tm_nbr'].mean()
        result = f"The average wait time is: {{average_wait_time}}"
        ```
        
        If the user asks: 'Which rows have more than average avg_wait_tm_nbr?' you should generate:
        ```python
        average_wait_time = data['avg_wait_tm_nbr'].mean()
        filtered_data = data[data['avg_wait_tm_nbr'] > average_wait_time]
        result = filtered_data
        ```

        IMPORTANT:
        - Only generate code that is safe to execute. Do not use any potentially harmful functions.
        - Structure the result in a clear and presentable format.
        - If the query involves aggregation, output the result in natural language.
        - If the query involves filtering or selecting data, output the result as a DataFrame.

        Now, generate the Python code to answer the user's query:
        """

        # Get the LLM to generate code
        generated_code = await self.llm.generate_analysis(query_prompt)

        # Execute the generated code (CAREFULLY!)
        # In a production environment, you would need to use a sandboxed environment here
        local_vars = {"data": data.copy(), "result": None}
        try:
            # Use 'exec' within a controlled environment
            exec(generated_code, {}, local_vars)
            result = local_vars["result"]
            if isinstance(result, pd.DataFrame):
                result = result.to_string()  # Convert DataFrame to string for display
        except Exception as e:
            result = f"Error executing generated code: {e}"

        return {
            "query": user_query,
            "generated_code": generated_code,
            "result": result,
        }

class ReportingSystem:
    """Main system to coordinate report generation"""

    def __init__(self):
        self.agents = {}
        self.data = None

    def add_agent(self, agent: ReportAgent):
        """Add a reporting agent to the system"""
        self.agents[agent.name] = agent
        logger.info(f"Added agent: {agent.name}")

    def load_data(self, log_table_path: str, fact_table_path: str):
        """Loads data from the log and fact tables (CSV files), joins them,
        and performs basic preprocessing.

        Args:
            log_table_path: Path to the log table CSV file.
            fact_table_path: Path to the fact table CSV file.
        """
        try:
            # Load the CSV files into pandas DataFrames
            log_df = pd.read_csv(log_table_path)
            fact_df = pd.read_csv(fact_table_path)

            # Data Cleaning (example):
            # Convert relevant columns to lowercase for consistent joining
            log_df.columns = log_df.columns.str.lower()
            fact_df.columns = fact_df.columns.str.lower()
            # Convert date columns to datetime objects
            date_columns = [col for col in log_df.columns if col.endswith(('_dt', '_dttm'))]
            for col in date_columns:
                log_df[col] = pd.to_datetime(log_df[col], errors='coerce')

            date_columns = [col for col in fact_df.columns if col.endswith(('_dt', '_dttm'))]
            for col in date_columns:
                fact_df[col] = pd.to_datetime(fact_df[col], errors='coerce')

            # Identify Common Columns for Joining:
            common_columns = list(set(log_df.columns).intersection(fact_df.columns))

            # Perform the join operation using the common columns
            merged_df = pd.merge(log_df, fact_df, on=common_columns, how="inner")

            # Ensure there are no duplicate columns after the join
            merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]

            self.data = merged_df
            logger.info("Data loaded and merged successfully")

        except Exception as e:
            logger.error(f"Error loading or merging data: {str(e)}")
            self.data = None

    async def generate_reports(self) -> Dict[str, Any]:
        """Generate reports from all agents"""
        if self.data is None:
            raise ValueError("No data loaded into the system")

        reports = {}
        for agent_name, agent in self.agents.items():
            try:
                logger.info(f"Generating report with {agent_name}")
                results = await agent.process_data(self.data.copy())
                reports[agent_name] = agent.format_output(results)
            except Exception as e:
                logger.error(f"Error generating report with {agent_name}: {str(e)}")

        return reports

async def main():
    """
    Main function to create the reporting system, add agents, load data,
    generate reports, print them, and save them to files.
    """
    # Create the reporting system
    system = ReportingSystem()

    # Add agents
    system.add_agent(SchemaAnalysisAgent("Schema Analyzer"))
    system.add_agent(DataTypeAnalysisAgent("Data Type Analyzer"))
    system.add_agent(
        NaturalLanguageQueryingAgent("Natural Language Querying Agent")
    )

    # Load data (replace with your actual file paths)
    log_table_path = "log-table.csv"  # Replace with the actual path
    fact_table_path = "fact-table.csv"  # Replace with the actual path
    system.load_data(log_table_path, fact_table_path)

    # Create the reports directory if it doesn't exist
    reports_dir = "reports"
    os.makedirs(reports_dir, exist_ok=True)

    # Generate reports
    reports = await system.generate_reports()

    # Print and save reports
    for agent_name, report in reports.items():
        print(f"\n=== {agent_name} Report ===")
        print(report)

        report_filename = os.path.join(
            reports_dir, f"{agent_name.lower().replace(' ', '_')}_report.txt"
        )
        with open(report_filename, "w") as f:
            f.write(report)

if __name__ == "__main__":
    asyncio.run(main())
